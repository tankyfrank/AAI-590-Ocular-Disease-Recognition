{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvH7HPC5YJRW",
        "outputId": "62f9993e-69e3-453f-e92a-7d9c4759b2b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-19 19:44:54.310968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742438694.353348     391 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742438694.362098     391 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1742438694.406488     391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1742438694.406553     391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1742438694.406555     391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1742438694.406557     391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-03-19 19:44:54.414449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "# Core libraries essential for data manipulation, analysis, training, exploration, and others\n",
        "import os\n",
        "import cv2\n",
        "import gdown\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from albumentations.core.transforms_interface import DualTransform\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we check to confirm if the ocular disease dataset currently exists in our specified location and proceed forward to either download the file or move onto the next chunk of code."
      ],
      "metadata": {
        "id": "AsTw82Qz2TgV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeHZJP2sYJRX",
        "outputId": "6e4fce15-a005-4d7e-839b-5a4fa8ea513c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ocular-disease-recognition.zip already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Google Drive File ID with filename zip file\n",
        "file_id = \"1oQ6Vy_HqZlVHnkFspgxMn0IcE__D8Kmh\"\n",
        "zip_filename = \"ocular-disease-recognition.zip\"\n",
        "extract_path = \"./ocular-disease-recognition\"\n",
        "\n",
        "# Checking if the file already exists\n",
        "if not os.path.exists(zip_filename):\n",
        "    print(f\"Downloading {zip_filename}...\")\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_filename, quiet=False)\n",
        "else:\n",
        "    print(f\"{zip_filename} already exists. Skipping download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a repeated function handling the extraction of the zip file. checking if the dataset has already been unzipped to the defined folder, it extracts all files from the zip archive to our directory to preprocess and manipulate."
      ],
      "metadata": {
        "id": "nRNSIBH42kwC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPqQds9BYJRX",
        "outputId": "10ef62a6-66d2-4600-d57e-626932da87a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction skipped: ./ocular-disease-recognition already exists.\n"
          ]
        }
      ],
      "source": [
        "# Check if dataset extraction file exists\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    print(f\"Extracting {zip_filename}...\")\n",
        "    with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Extraction complete! Files extracted to: {extract_path}\")\n",
        "else:\n",
        "    print(f\"Extraction skipped: {extract_path} already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we load in the dataset from a csv file into a DataFrame and apply a filter to ensure that each record has both left and right fundus images available. Given the dual input nature of our model, this is essential to provide long term results. After filtering, we split the dataset into training, validation, and tests sets for model training and evaluation down the pipeline."
      ],
      "metadata": {
        "id": "92qgKjJn3CG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7D9LQM9YJRY"
      },
      "outputs": [],
      "source": [
        "# Load the dataset, update the path for local machine\n",
        "dataset_path = \"processed_ocular_disease.csv\"\n",
        "# Read in CSV file to dataframe\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Filter dataset for only instances having both left and right fundus images present\n",
        "df = df[\n",
        "    df.apply(lambda row:\n",
        "        os.path.exists(os.path.join('ocular-disease-recognition/preprocessed_images', row['Left-Fundus'])) and\n",
        "        os.path.exists(os.path.join('ocular-disease-recognition/preprocessed_images', row['Right-Fundus'])),\n",
        "        axis=1\n",
        "    )\n",
        "].reset_index(drop=True)\n",
        "\n",
        "# Dataset split into training and temporary datasets for training and validation\n",
        "# 70% Training, 30% Temporary\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "# 15% Validation, 15% Testing\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the large data imbalance present in our dataset, we approach the problem with class weights via SKLearn. The weights are calculated in order to counteract any imbalance in the dataset for training stability and classification ability of our model"
      ],
      "metadata": {
        "id": "jkbD-i4D3X30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_OVuRnIYJRY",
        "outputId": "8dd011c7-4c97-458b-f8dc-3e547001941d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 8\n",
            "Computed Class Weights: {0: np.float64(2.917307692307692), 1: np.float64(2.718637992831541), 2: np.float64(0.49285250162443145), 3: np.float64(2.7783882783882783), 4: np.float64(6.01984126984127), 5: np.float64(3.77363184079602), 6: np.float64(0.2791682002208318), 7: np.float64(1.1270430906389302)}\n"
          ]
        }
      ],
      "source": [
        "# Calculate number of unique class labels\n",
        "num_classes = len(np.unique(df['labels']))\n",
        "print(f\"Number of Classes: {num_classes}\")\n",
        "\n",
        "# Compute class weights for class imbalance\n",
        "class_labels = np.unique(df['labels'])\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=class_labels, y=df['labels'])\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_labels))}\n",
        "print(\"Computed Class Weights:\", class_weight_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We continue with preprocessing by rebalancing the training dataset, utilizing resampling with replacement on each class so that all classes have an equal number of samples. The balanced dataset is shuffled with counts of each label printing to very class balances."
      ],
      "metadata": {
        "id": "7-Wg3G5YCpS6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQAkGFbKYJRY",
        "outputId": "dd195b97-d7ce-45b6-c089-f1689fea75f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "labels\n",
            "3    1901\n",
            "0    1901\n",
            "6    1901\n",
            "2    1901\n",
            "7    1901\n",
            "4    1901\n",
            "5    1901\n",
            "1    1901\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# function to rebalance classes (?)\n",
        "def balance_classes(df):\n",
        "    \"\"\"Resample dataset to balance classes.\"\"\"\n",
        "    max_size = df['labels'].value_counts().max()\n",
        "    balanced_df = pd.concat([\n",
        "        resample(df[df['labels'] == cls], replace=True, n_samples=max_size, random_state=42)\n",
        "        for cls in df['labels'].unique()\n",
        "    ])\n",
        "    # Shuffle after resampling.\n",
        "    return balanced_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Apply to training data only\n",
        "train_df_balanced = balance_classes(train_df)\n",
        "\n",
        "# Check if balancing worked\n",
        "print(train_df_balanced['labels'].value_counts())  # Should now be balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving along, we create a custom data pipeline to handle paired image data and augmentations with our DualImageAugmentation class to ensure that both images per instance undergo the same random transformations. Alongside this, a customd ata generator using Keras Sequence class was used to load images and apply augmentations, merging the two resulting greyscale images into a 2-channel input and outputting batches of data alongwith their corresponding labels."
      ],
      "metadata": {
        "id": "qnywucQeD5nB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1gKsVjxYJRY"
      },
      "outputs": [],
      "source": [
        "# Custom dual augmentation class inherintg albumentations\n",
        "class DualImageAugmentation(DualTransform):\n",
        "    def __init__(self, transforms, always_apply=False, p=0.5):\n",
        "        # Initialize parent class store augmentation pipeline\n",
        "        super(DualImageAugmentation, self).__init__(always_apply, p)\n",
        "        self.transforms = A.Compose(transforms)\n",
        "\n",
        "    def apply(self, img, **params):\n",
        "        # Apply pipeline to image and return transformations\n",
        "        return self.transforms(image=img)[\"image\"]\n",
        "\n",
        "    def apply_to_image1(self, img, **params):\n",
        "        # Specifically apply same transformation to second paired image\n",
        "        return self.transforms(image=img)[\"image\"]\n",
        "\n",
        "# Ocular data generator\n",
        "class OcularDatasetGenerator(Sequence):\n",
        "    def __init__(self, df, batch_size=32, img_size=(128, 128), shuffle=True, augment=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Filter dataframe to include only rows with existing left and right image pairs\n",
        "        self.df = df[df.apply(lambda row:\n",
        "            os.path.exists(os.path.join('ocular-disease-recognition/preprocessed_images', row['Left-Fundus'])) and\n",
        "            os.path.exists(os.path.join('ocular-disease-recognition/preprocessed_images', row['Right-Fundus'])),\n",
        "            axis=1\n",
        "        )].reset_index(drop=True)  # Reset index after filtering\n",
        "        print(f\"Dataset initialized with {len(self.df)} valid samples.\")\n",
        "\n",
        "\n",
        "        # Set class attributed based on the provided parameters\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(df))\n",
        "\n",
        "        # Define augmentation pipeline if augmentation is enabled\n",
        "        if augment:\n",
        "            self.augmentation_pipeline = self.get_augmentation_pipeline()\n",
        "        else:\n",
        "            self.augmentation_pipeline = None\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Get indices for the current batch based on batch size\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        # SElect corresponding rows\n",
        "        batch = self.df.iloc[batch_indices]\n",
        "        # Generate batch data for images and labels\n",
        "        X, y = self.__data_generation(batch)\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def __data_generation(self, batch):\n",
        "        # Initialize empty lists to accumulate batch images and labels\n",
        "        X_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "        # Iterate over each row\n",
        "        for _, row in batch.iterrows():\n",
        "            # Construct full file paths for left and right fundus images\n",
        "            left_image_path = os.path.join('ocular-disease-recognition/preprocessed_images', row['Left-Fundus'])\n",
        "            right_image_path = os.path.join('ocular-disease-recognition/preprocessed_images', row['Right-Fundus'])\n",
        "\n",
        "            # Load images using helper method\n",
        "            left_image = self.load_image(left_image_path)\n",
        "            right_image = self.load_image(right_image_path)\n",
        "\n",
        "            # Skip iteration if either image could not be loaded\n",
        "            if left_image is None or right_image is None:\n",
        "                continue  # Skip invalid images\n",
        "\n",
        "            # Apply augmentation (both images get the same transformation)\n",
        "            if self.augment and self.augmentation_pipeline:\n",
        "                augmented = self.augmentation_pipeline(image=left_image, image1=right_image)\n",
        "                left_image = augmented[\"image\"]\n",
        "                right_image = augmented[\"image1\"]\n",
        "\n",
        "            # Convert grayscale images to 3D (required for CNN)\n",
        "            left_image = np.expand_dims(left_image, axis=-1)\n",
        "            right_image = np.expand_dims(right_image, axis=-1)\n",
        "\n",
        "            # Merge images into a two-channel input\n",
        "            combined_image = np.concatenate((left_image, right_image), axis=-1)\n",
        "\n",
        "            X_batch.append(combined_image)\n",
        "            y_batch.append(int(row['labels']))\n",
        "\n",
        "        return np.array(X_batch, dtype=np.float32), np.array(y_batch, dtype=np.int32)\n",
        "\n",
        "\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        # Read image in grayscale via CV2\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            return None\n",
        "        # Resize image to target dimensions\n",
        "        image = cv2.resize(image, self.img_size)\n",
        "        # normalize pixel values to 0, 1 range\n",
        "        image = image / 255.0\n",
        "        return image\n",
        "\n",
        "    def get_augmentation_pipeline(self):\n",
        "        # Define a composition of augmentation transofrmations\n",
        "        return A.Compose([\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Rotate(limit=30, p=0.5),\n",
        "            A.ElasticTransform(p=0.5),\n",
        "            A.CoarseDropout(max_holes=3, max_height=0.2, max_width=0.2, p=0.5),\n",
        "        ], additional_targets={\"image1\": \"image\"})\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\" Shuffle indices at the end of each epoch. \"\"\"\n",
        "        self.indices = np.arange(len(self.df))  # Ensure indices match filtered dataset\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving forward, we now compile and build our model on 64 batch sizing across 237 epochs, setting our metric of interest as accuracy."
      ],
      "metadata": {
        "id": "d0It6KJ1HPr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J-notfXYJRY",
        "outputId": "5f4db7ba-662a-45c4-c98c-5468cc3bf237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset initialized with 15208 valid samples.\n",
            "Dataset initialized with 910 valid samples.\n",
            "Dataset initialized with 911 valid samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_391/3805339739.py:165: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(max_holes=3, max_height=0.2, max_width=0.2, p=0.5),\n",
            "I0000 00:00:1742438785.377546     391 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8847 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1742438793.306045     752 service.cc:152] XLA service 0x7f74a4004d20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1742438793.306183     752 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
            "2025-03-19 19:46:33.430513: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1742438794.097146     752 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "2025-03-19 19:46:35.293984: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4658', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m  1/237\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07:55\u001b[0m 17s/step - accuracy: 0.0938 - loss: 3.3140"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1742438806.206409     752 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 254ms/step - accuracy: 0.2376 - loss: 2.4636 - val_accuracy: 0.0949 - val_loss: 2.7261 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 235ms/step - accuracy: 0.3270 - loss: 1.9466 - val_accuracy: 0.0525 - val_loss: 3.6943 - learning_rate: 9.9975e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 226ms/step - accuracy: 0.3961 - loss: 1.6655 - val_accuracy: 0.1908 - val_loss: 1.9448 - learning_rate: 9.9901e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 237ms/step - accuracy: 0.4660 - loss: 1.4428 - val_accuracy: 0.2377 - val_loss: 1.7995 - learning_rate: 9.9778e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 220ms/step - accuracy: 0.5132 - loss: 1.3165 - val_accuracy: 0.2589 - val_loss: 1.7174 - learning_rate: 9.9606e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 217ms/step - accuracy: 0.5481 - loss: 1.2080 - val_accuracy: 0.3147 - val_loss: 1.6692 - learning_rate: 9.9384e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 217ms/step - accuracy: 0.5937 - loss: 1.0974 - val_accuracy: 0.2924 - val_loss: 1.7421 - learning_rate: 9.9114e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 220ms/step - accuracy: 0.6170 - loss: 1.0391 - val_accuracy: 0.2455 - val_loss: 1.9066 - learning_rate: 9.8796e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 219ms/step - accuracy: 0.6100 - loss: 1.0706 - val_accuracy: 0.3013 - val_loss: 1.6941 - learning_rate: 9.8429e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 232ms/step - accuracy: 0.6457 - loss: 0.9456 - val_accuracy: 0.3359 - val_loss: 1.5632 - learning_rate: 9.8015e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 265ms/step - accuracy: 0.6786 - loss: 0.8730 - val_accuracy: 0.3170 - val_loss: 1.6493 - learning_rate: 9.7553e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 252ms/step - accuracy: 0.7170 - loss: 0.7669 - val_accuracy: 0.4007 - val_loss: 1.4947 - learning_rate: 9.7044e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 223ms/step - accuracy: 0.7205 - loss: 0.7412 - val_accuracy: 0.3917 - val_loss: 1.5634 - learning_rate: 9.6489e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 221ms/step - accuracy: 0.7232 - loss: 0.7505 - val_accuracy: 0.4208 - val_loss: 1.4743 - learning_rate: 9.5888e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 219ms/step - accuracy: 0.7336 - loss: 0.7126 - val_accuracy: 0.3962 - val_loss: 1.6253 - learning_rate: 9.5241e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 220ms/step - accuracy: 0.7625 - loss: 0.6419 - val_accuracy: 0.3951 - val_loss: 1.5768 - learning_rate: 9.4550e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 220ms/step - accuracy: 0.7524 - loss: 0.6642 - val_accuracy: 0.4475 - val_loss: 1.5106 - learning_rate: 9.3815e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 219ms/step - accuracy: 0.7656 - loss: 0.6384 - val_accuracy: 0.4118 - val_loss: 1.5936 - learning_rate: 9.3037e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 223ms/step - accuracy: 0.7845 - loss: 0.5806 - val_accuracy: 0.3973 - val_loss: 1.6955 - learning_rate: 9.2216e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 217ms/step - accuracy: 0.7917 - loss: 0.5803 - val_accuracy: 0.4420 - val_loss: 1.5719 - learning_rate: 9.1354e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 218ms/step - accuracy: 0.8120 - loss: 0.5151 - val_accuracy: 0.4531 - val_loss: 1.5709 - learning_rate: 9.0451e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 216ms/step - accuracy: 0.8060 - loss: 0.5348 - val_accuracy: 0.4754 - val_loss: 1.4930 - learning_rate: 8.9508e-04\n"
          ]
        }
      ],
      "source": [
        "# **Create Data Generators**\n",
        "batch_size = 64\n",
        "train_generator = OcularDatasetGenerator(train_df_balanced, batch_size=batch_size, img_size=(224, 224), augment=True)\n",
        "val_generator = OcularDatasetGenerator(val_df, batch_size=batch_size, img_size=(224, 224))\n",
        "test_generator = OcularDatasetGenerator(test_df, batch_size=batch_size, shuffle=False, img_size=(224, 224))\n",
        "\n",
        "\n",
        "# Build a sequential convolutional NN\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(224, 224, 2)),\n",
        "\n",
        "    # Convolutional block\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    # Batch normalization\n",
        "    layers.BatchNormalization(),\n",
        "    # Downsampling feature maps\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    # Dropout 10% of nodes\n",
        "    layers.Dropout(0.1),\n",
        "\n",
        "    # Convolutional block 2\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    #Convolutional block 3 with increased filters\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Convolutional block 4: Increasing filters to 256\n",
        "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Convolutional block 5: filter increased to 512\n",
        "    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    # Flatten output to 1D vector\n",
        "    layers.Flatten(),\n",
        "    # Dense layer with 512 units and ReLU activation\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    # Highdropout of 50%\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Defined learning rate scheduler for step decay\n",
        "def step_decay(epoch):\n",
        "    initial_lr = 0.001\n",
        "    drop = 0.5\n",
        "    epochs_drop = 5\n",
        "    return initial_lr * (drop ** (epoch // epochs_drop))\n",
        "\n",
        "# Another learning rate scheduler with coside decay\n",
        "def cosine_decay(epoch):\n",
        "    initial_lr = 0.001\n",
        "    return initial_lr * (0.5 * (1 + np.cos(np.pi * epoch / 100)))\n",
        "\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(cosine_decay)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',  # ✅ Use sparse version\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "# **Define Early Stopping Callback**\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',    # Stop if validation loss stops improving\n",
        "    patience=8,            # Wait for 5 epochs before stopping\n",
        "    restore_best_weights=True  # Restore best model weights\n",
        ")\n",
        "\n",
        "# **Train the Model with Early Stopping**\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, lr_schedule],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x_voTxnYJRZ",
        "outputId": "3b194eef-0fde-4811-99bc-bbac2450daef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 217ms/step - accuracy: 0.4259 - loss: 1.4151\n",
            "Test Accuracy: 0.4241\n"
          ]
        }
      ],
      "source": [
        "# **Evaluate the Model on Test Set**\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite best attempts, the model only attained 42% accuracy in this iteration, with a high of 47% accuracy in previous training attempts. However, this proves to be an insufficient approach, from which we developed our final model architecture present in ResNetMultiModel_v4.ipynb notebook instead."
      ],
      "metadata": {
        "id": "dQsDJUtbHXeK"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}